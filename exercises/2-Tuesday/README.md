# Tuesday Exercises: Neural Network Optimization

## Overview

These exercises reinforce backpropagation mechanics, gradient descent optimization, and batch normalization covered in today's demos. You'll implement gradient computations manually and experiment with different training strategies.

## Exercise Schedule

| Exercise | Type | Duration | Prerequisites |
|----------|------|----------|---------------|
| 01: Backpropagation by Hand | Hybrid | 60 min | Demo 01, Readings 01-02 |
| 02: Optimizer Comparison | Implementation | 45 min | Demo 02, Reading 02 |
| 03: Batch Normalization Experiment | Implementation | 60 min | Demos 03-04, Reading 04 |
| 04: Custom Callbacks | Implementation | 45 min | Demo 04, Reading 05 |

## Prerequisites

- Completed readings: `01-05` in `readings/2-Tuesday/`
- Watched demos: `demo_01` through `demo_04`
- Python environment with TensorFlow 2.x, NumPy, Matplotlib

## Learning Objectives

By completing these exercises, you will be able to:

1. Manually compute gradients using the chain rule
2. Compare and select appropriate optimizers for different tasks
3. Implement and evaluate batch normalization effects
4. Build custom training loops with gradient control

## Getting Started

```bash
cd exercises/2-Tuesday
pip install -r requirements.txt
```

Start with Exercise 01 and proceed in order.

## Guidance Philosophy

- **Exercise 01**: Heavily guided with step-by-step gradient derivations
- **Exercise 02**: Moderate guidance with framework provided
- **Exercise 03**: Minimal guidance - design your own experiments
